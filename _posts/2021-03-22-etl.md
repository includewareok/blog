---
title: "procesos ETL"
excerpt_separator: "<!--more-->"
permalink: /etl-101
modified: 2021-03-22   
classes: wide
comments: true
categories:
  - "ingeniería-de-datos"
tags:
  - "Data Engineer"
---

![visitors](https://visitor-badge.glitch.me/badge?page_id=includewareok.blog.2021-03-10-etl")

## Introducción

Hace un tiempo leí un artículo que hacía una comparativa sobre la `ingeniería de datos` y la `ingeniería civil`, en el mismo se describían 4 piezas fundamentales para realizar de forma satisfactoria esto. El barrio en el que estoy trabajando hay muchas obras y desde la ventana de mi apartamento puedo ver y relacionar estas dos áreas de ingeniería
* La materia prima debe ser llevada al lugar de trabajo. Veo que llegan varios camiones con cemento algunos días y otros vienen con varillas de hierro, otros vienen camiones llenos de maderas.
* Los materiales deben ser cortados y modificados con un propósito y almacenados en algún lugar. Hubo una semana, que estuvieron todos los días, doblando, cortando y soldando las varillas para armar las diferentes estructuras y después las iban guardando bajo un techo.
* El edificio real es el nuevo `modelo ML` o `insights` 
* El supervisor dirige todos los aspectos y el equipo del proyecto. 

Hoy vamos a hablar de los primeros 2 temas.
<!--more-->

## Gestión de datos
Por lo general, los datos están desparramados en diferentes **silos** y debemos dejarlos en un mismo lugar para poder después aplicar algún tipo de procesamiento. Para tener una buena gestión de nuestros datos, debemos contar con un buen proceso para los mismos, siguiendo el ejemplo anterior, se imaginan empezar a construir pilares con un acero que es de mala calidad o vino con defectos? ¿Qué posibilidades tenemos de que nuestro proyecto salga mal? De la misma forma, no podemos llevar todos los datos a nuestro almacén sin preocuparnos por algunos detalles.

* ¿Donde están ahora? Primero tenemos que saber donde están los datos que vamos a usar, son logs de una app o de in web server, están en un contenedor o una VM? 
* ¿Qué tan grande son? Son pequeños archivos de 100 o 200 MB o estamos hablando de GB o TB de datos.
* ¿Qué estructura tienen los datos? Debemos saber como están estructurados, si es que tiene alguna, nuestros datos para saber si debemos hacer un pre procesamiento.
* ¿A donde los tengo que llevar? Esto es importante para definir los procesos, porque si vamos a ir por una internet o algún enlace lento, este proceso puede llevar más tiempo.
* ¿Qué cantidad de transformación requiere los datos? Hablaremos más en detalle más adelante, pero básicamente es importante saber si me sirven los datos como están en el origen o debo aplicarle operaciones para dejarlos apropiados.

## Calidad de los datos
Un punto importante, qué si bien tiene que ver con la gestión de los datos, es la calidad de los mismos. Esto va afectar el `data pipeline` por completo porque necesitamos aplicar cambios en los mismos. Por ejemplo, si tengo un sensor de temperatura corporal y que se los valores deben estar entre 30°C y 45°C si recibo un valor de 90°C, estoy seguro qué es un error de lectura y por lo tanto debo marcar de alguna forma para que no nos afecte nuestro modelo. Otras veces se quieren mantener el registro porque si bien le falta una columna, las otras son igualmente validas, para estos casos se emplean diferentes técnicas, por ejemplo, agregar el valor inmediatamente anterior o posterior o poner con el valor medio de toda la columna. Todo esto se investiga en una etapa de la ingeniería de datos llamada etapa de exploración de datos o `DEA` por sus siglas en inglés. 

### ¿Cómo manejamos los no valores?
Existe también un tema de los valores vacíos. Dependiendo el sistema, los valores faltantes son tratados de diferentes formas. Por ejemplo, un valor `null` es que no se han cargado datos, pero a veces se usan valores fuera del rango, por ejemplo, si el rango es de 0 a 100, un valor de -1 es lo mismo que sin valor. Entonces es importante saber que significa que un valor no está, es algo por diseño o es un error. En base a eso debemos elegir que hacer para sustituir dichos valores.


### Procesamiento por batch vs procesamiento real-time
También debemos tener en cuenta como que frecuencia vamos a procesar nuestros datos. Los vamos va a procesar 1 sola vez por vez los datos, capaz al finalizar las ventas, o va a ir procesando datos a medida que llegan. Este punto que parece trivial en verdad nos define un montón de aspecto, por ejemplo la estrategia a utilizar pero también la capacidad que debemos planear. No es lo mismo procesar pequeños bloques de datos que un volumen grande porque requieren diferente cantidad de memoria y estrategia de pre-almacenado. Por último, también nos define la disponibilidad de la información, en el proceso por batch debemos esperar para poder analizar los datos, mientas que en el procesamiento en tiempo real, están prontos en el momento

Teniendo todo esto en cuenta, tenemos básicamente 3 operaciones para realizar con nuestros datos en la etapa de ingreso, `ingest` en inglés.
* _Extract_: La **E** hace referencia a la operación de extración de datos, significa que tengo un dato y lo extraigo de un origen 
* _Transform_: La **T** hace referencia a las técnicas que nombramos antes, transoforman el dato de alguna forma, en general son operaciones simples.
* _Load_: La **L** por `load` en inglés, hace referencia a la operación de carga del dato a nuestro almacén, acá hablo de almacén y no específicamente que tipo porque eso depende de nuestra estrategia.

**ETL**
 ![ETL](/assets/images/2021-03/22/etl.png) 
 
 **ELT** 
 ![ELT](/assets/images/2021-03/22/elt.png) |


**:information_source:** 
Imágenes obtenidas del [sitio de documentación de Microsoft](https://docs.microsoft.com/es-es/azure/architecture/data-guide/relational-data/etl)
{: .notice}


Ahora veamos las diferentes formas de hacerlo.

### Diferentes casos
Vamos a describir los diferentes casos posibles en base a lo anterior:
* _EL_: El caso más simple es cuando simplemente extraemos un dato y lo llevamos a otro lugar, esto o bien puede ser porque tenemos el mismo esquema-normalizado en el sistema original y el que va a hacer el análisis o porque por ahora solo nos interesa tener una copia del dato en si, por ejemplo, tenemos rotación de logs cada 7 días, pero queremos poder hacer un análisis en un periodo más largo de tiempo.
* _ETL_: El siguiente caso es el que le da nombre a esta etapa y suma a la anterior, una transformación en el medio. Esta transformación puede ser bien por necesidades del negocio, para mejorar la salud de los datos o normalizar los datos.
* _ELT_: Es un caso similar al anterior, pero la etapa de transformación se realiza ya en el destino.
* _ETLT_: Es una mezcla de los anteriores dos, hace una transformación  inicial antes de cargar y una posterior.

### Cuando usar cual
Habiendo visto las diferentes formas de realizar estos procesos, debemos plantearnos entonces, ¿Cual usar? y ¿Cuándo usarlo? Por lo tantos vamos a preferir:
* EL: Cuando los datos ya cuentan con buena calidad y comparten una normalización entre los sistemas de origen y destino.
* ETL: Nos importa la calidad de los datos que estamos almacenando y aplicar una capa de seguridad _enmascarador_ o _anonimización_ de los mismos.
* ELT: Cuando el volumen de datos ingresados es mayor a nuestra capacidad de procesarlos y corremos riesgo de perder datos. También puede ser cuando queremos tener más datos para crear o mejorar los modelos de ML o poder detectar relaciones que no son visibles desde un punto de vista humano.
* ETLT: Cuando la primera transformación es simple o sencilla, por ejemplo, enmascarado de datos u operaciones simples, agregar un pre o post fijo a una cadena y la segunda transformación pueden ser agrupaciones complejas o detección de anomalías en los conjuntos.


### Conclusiones
El proceso de ingesta de datos es una etapa de la ingeniería de datos que se encarga de obtener los datos de los diferentes sistemas y consolidarlos en  un lugar para un posterior procesamiento. En general desde un punto de vista de arquitectura de alto nivel, se ubican entre los procesos de OLTP y OLAP. En general desde un punto de vista de arquitectura de alto nivel, se ubican entre los procesos de OLTP y OLAP. Además, es importante saber de donde vienen y a donde van nuestros datos, que tipo estrategias vamos a tomar para los no valores. Por último, también tenemos que saber si vamos a procesar por lotos los datos o en tiempo real.