---
title: "procesos ETL"
excerpt_separator: "<!--more-->"
permalink: /etl-101
modified: 2021-03-22   
classes: wide
comments: true
categories:
  - "ingenieria-de-datos"
tags:
  - "Data Engineer"
---

![visitors](https://visitor-badge.glitch.me/badge?page_id=includewareok.blog.2021-03-10-etl")

## Introducción

Hace un tiempo leí un articulo que hacía una comparativa sobre la `ingenieria de datos` y la `ingenieria civil`, en el mismo se describian 4 piesas fundamentales para realizar de forma satisfactoria esto. El barrio en el que estoy trabajando hay muchas obras y desde la ventana de mi apartamento puedo ver y relacionar estas dos áreas de ingenieria
* La materia prima debe ser llevada al lugar de trabajo. Veo que llegan varios camiones con cemento algunos días y otros vienen con barillas de hierro, otros vienen camiones llenos de maderas.
* Los materiales debe ser cortados y modificados con un proposito y almacenados en algún lugar. Hubo una semana, que estuvieron todos los días, doblando, cortando y soldando las varillas para armar las diferentes esctrucutras y después las iban guardando bajo un techo.
* El edificio real es el nuevo `modelo ML` o `insights` 
* El supervior dirige todos los aspectos y el equipo del proyecto. 

Hoy vamos a hablar de los primeros 2 temas.
<!--more-->

## Gestión de datos
Por lo general, los datos están desparramados en diferentes **silos** y debemos dejarlos en un mismo lugar para poder después aplicar algun tipo de procsamiento. Para tener una buena gestión de nuestros datos, debemos contar con un buen proceso para los mismos, siguiendo el ejemplo anterior, se imaginan empezar a construir pilares con un acero que es de mala calidad o vino con defectos? ¿Qué posibildiades tenemos de que nusetro proyecto salga mal? De la misma forma, no podemos llevar todos los datos a nuestro almacen sin preocuparnos por alguns detalles.

* ¿Donde están ahora?: Primero tenemos que saber donde están los datos que vamos a usar, son logs de una app o de in web server, están en un contenedor o una VM? 
* ¿Qué tan grande son? : Son pequeños archivos de 100 o 200 MB o estamos hablando de GB o TB de datos
* ¿Qué estructura tienen los datos? Debemos saber como están estructurados, si es que tiene alguna, nuestros datos para saber si debemos hacer un pre procesamiento
* ¿A donde los tengo que llevar? Esto es importante para definir los procesos, porque si vamos a ir por una internet o algún enlace lento, este proceso puede llevar más tiempo.
* ¿Qué cantidad de transoformación requiren los datos? Hablaremos más en detalla más adelante, pero básicamente es importante saber si me sirven los datos como están en el origen o debo aplicarle operaciones para dejarlos apropiados.

## Calidad de los datos
Un punto importante, que si bien tiene que ver con la gestión de los datos, es la calidad de los mismos. Esto va afectar el `data pipeline` por completo porque necesitamos aplicar cambios en los mismos. Por ejemplo, si tengo un sensor de temperatura corporal y que se los valores deben estar entre 30°C y 45°C si recibo un valor de 90°C, estoy seguro que es un error de lectura y por lo tanto debo marcar de alguna forma para que no nos afecte nuestro modelo. Otras veces se quieren mantener el registro porque si bien le falta una columna, las otras son igualmente validas, para estos casos se emplean diferentes técnicas, por ejemplo agregar el valor inmediatamente anterior o posterior o poner con el valor medio de toda la columna. Todo esto se investiga en una etapa de la ingenieria de datos llamada etapa de exploración de datos o `DEA` por sus siglas en inglés. 

### ¿Cómo manejamos los no valores?
Existe también un tema de los valores vacios. Dependiendo el sistema, los valores faltantes son tratados de diferentes formas. Por ejemplo, un valor `null` es que no se han cargado datos, pero a veces se usan valores fuera del rango, por ejemplo, si el rango es de 0 a 100, un valor de -1 es lo mismo que sin valor. Entonces es importante saber que significa que un valor no está, es algo por diseño o es un error. En base a eso debemos elegir que hacer para sustituir dichos valores.


### Procesamiento por batch vs procesamiento real-time
También debemos tener en cuenta como vamos a procesar nuestros datos. Nuestro sistema va a procesar 1 sola vez por vez los datos, capaz al finalizar las ventas, o va a ir procesando datos a medida que llegan. 

Teniendo todo esto en cuenta, tenemos básicamente 3 operaciones para realizar con nuestros datos en la etapa de ingreso, `ingest` en inglés.
* _Extract_: La **E** hace referencia a la operación de extración de datos, significa que tengo un dato y lo extraigo de un origen 
* _Transform_: La **T** hace referencia a las técnicas que nombramos antes, transoforman el dato de alguna forma, en general son operaciones simples.
* _Load_: La **L** por `load` en inglés, hace referencia a la operación de carga del dato a nuestro almacen, acá hablo de almacen y no especificamente que tipo porque eso depende de nuestra estrategía.

| ETL | ELT |
|---|---|
| ![ETL](/assets/images/2021-03/22/etl.png) |  ![ELT](/assets/images/2021-03/22/elt.png) |

**:information_source:** 
Imagenes obtenidas del [sitio de documentación de Microsoft](https://docs.microsoft.com/es-es/azure/architecture/data-guide/relational-data/etl)
{: .notice}


Ahora veamos las diferentes formas de hacerlo.

### Diferentes casos
Vamos a describir los diferentes casos posibles en base a lo anterior:
* _EL_: El caso más simple es cuando simplemento extraemos un dato y lo llevamos a otro lugar, esto o bien puede ser porque tenemos el mismo esquema-normalizado en el sistema original y el que va a hacer el analisis o porque por ahora solo nos interesa tener una copia del dato en si, por ejemplo, tenemos rotación de logs cada 7 días pero queremos poder hacer un analisis el un periodo más largo de tiempo.
* _ETL_: El siguiente caso es el que le da nombre a esta etapa y suma a la anterior, una transofmración en el medio. Esta transofmación puede ser bien por necesidades del negocio, para mejorar la salud de los datos o normalizar los datos.
* _ELT_: Es un caso similar al anterior, pero la etapa de transformación se realiza ya en el destino.
* _ETLT_: Es una mezcla de los anteriores dos, hace una transoformación inicial antes de cargar y una posterior.

### Cuando usar cual
Habiendo visto las diferentes formas de realizar estos procesos, debemos plantearnos entonces, ¿Cual usar? y ¿Cúando? Por lo tantos vamos a preferir por ejemplo
* EL: Cuando los datos ya cuentan con buenca calidad y comparten una normalización entre los sistemas de origen y destino.
* ETL: Nos importa la calidad de los datos que estamos almacenando y aplicar una capa de seguridad _enmascarador_ o _anonimización_ de los mismos.
* ELT: Cuando el volumen de datos ingeresados es mayor a nuestra capacidad de proecsarlos y corremos riesgo de perder datos. También puede ser cuando queremos tener más datos para crear o mejorar los modelos de ML o poder detectar relaciones que no son visibles desde un punto de vista humano.
* ETLT: Cuando la primera transoformación es simple o sencilla, por ejemplo, enmascarado de datos u operaciones simples, agregar un pre o post fijo a una cadena y la segunda transofrmación pueden ser agrupaciones complejas o detección de anomalias en conjuntos


### Conclusiones
El proceso de ingesta de datos, es una etapa de la ingenieria de datos que se encarga de obtener los datos de los diferentes sistemas y consolidarlos en  un lugar para un posterior procesamiento. En general desde un punto de vista de arquitectura de alto nivel, se ubican entre los procesos de OLTP y OLAP. Además, es importante saber de donde vienen y a donde van nuestros datos, que tipo estrategias vamos a tomar para 